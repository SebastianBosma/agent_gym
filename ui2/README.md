# RL Post-Training Demo

A Next.js web application that demonstrates reinforcement learning post-training using offline dialogue traces. The app uses Google Gemini models to optimize system prompts (policies) for a dialogue assistant through a simple ε-greedy bandit algorithm.

## Features

- **Teacher Model**: `gemini-3-pro-preview` (LLM-as-a-judge / rubric generator)
- **Student Model**: `gemini-2.0-flash-lite` (assistant being "post-trained")
- **Environment**: Offline DSTC8-style dialogue dataset
- **Post-training**: Multi-armed bandit over system prompts
- **Real-time UI**: Chat-like interface showing training progress
- **Visualization**: Policy stats table and reward progression chart
- **Comparison**: Before/after evaluation on held-out examples

## Prerequisites

- Node.js 20.9.0 or later
- A Google API key with access to Gemini models

## Setup

1. **Install dependencies:**
   ```bash
   npm install
   ```

2. **Set up environment variables:**
   
   Copy `.env.example` to `.env.local`:
   ```bash
   cp .env.example .env.local
   ```
   
   Then edit `.env.local` and add your Google API key:
   ```
   GOOGLE_API_KEY=your_actual_api_key_here
   ```
   
   Get your API key from: https://aistudio.google.com/app/apikey

3. **Run the development server:**
   ```bash
   npm run dev
   ```

4. **Open the app:**
   
   Navigate to [http://localhost:3000](http://localhost:3000) in your browser.

## How It Works

### Architecture

The app uses Next.js App Router with server-side API routes for all Gemini model calls:

- `app/page.tsx` - Main UI with chat window and side panel
- `app/api/train/route.ts` - Training endpoint (runs N bandit steps)
- `app/api/compare/route.ts` - Comparison endpoint (before/after evaluation)
- `lib/teacher.ts` - Teacher model (rubric generation + reward scoring)
- `lib/student.ts` - Student model (generate responses)
- `lib/banditTrainer.ts` - ε-greedy bandit logic
- `lib/policies.ts` - System prompt definitions
- `data/env_sgd.jsonl` - Offline dialogue dataset

### Policies

The app tests multiple system prompts (policies):

1. **Baseline**: Standard helpful assistant
2. **Concise**: Brief, precise responses
3. **Empathetic**: Acknowledges user feelings
4. **Clarifying**: Asks questions when needed

### Training Process

1. Sample a random dialogue turn from the dataset
2. Use ε-greedy to select a policy (system prompt)
3. Generate rubric for evaluation (if not cached)
4. Student model generates response using selected policy
5. Teacher model scores the response (0-1)
6. Update policy statistics
7. Repeat for N steps

### UI Components

- **ChatWindow**: Shows step-by-step training logs
- **ControlsPanel**: Configure training parameters (steps, epsilon)
- **PolicyStatsTable**: Displays trials and avg reward per policy
- **RewardChart**: Line chart of best avg reward vs step
- **CompareViewer**: Shows baseline vs best policy comparison

## Usage

1. **Run Training:**
   - Set number of steps (10, 20, or 50)
   - Adjust epsilon (exploration rate: 0.0 - 0.4)
   - Click "Run Training"
   - Watch the chat fill with step-by-step progress

2. **View Results:**
   - Policy stats update in real-time
   - Chart shows reward progression
   - Best policy is highlighted

3. **Compare Before/After:**
   - Click "Compare Before vs After"
   - See baseline vs best policy on a held-out example
   - View teacher scores for both

4. **Reset:**
   - Click "Reset" to start over

## Dataset

The app includes a small subset of DSTC8 Schema-Guided Dialogue data (`data/env_sgd.jsonl`). Each line contains:

- `id`: Unique identifier
- `dialogue_id`: Conversation ID
- `turn_index`: Turn number
- `prompt`: Dialogue history up to this turn
- `ground_truth`: Reference response
- `rubric`: Evaluation criteria (generated by teacher)
- `task_description`: Context about the user's goal

## Technical Details

- **No local GPUs or fine-tuning**: Only API calls to Gemini
- **Server-side only**: API keys never exposed to client
- **Stateless**: No database, all state in memory during session
- **Fast demo**: Small dataset (10-20 examples) for quick iteration
- **Bandit optimization**: Simple but effective for prompt search

## Limitations

- In-memory state (resets on server restart)
- No persistent storage of optimized policies
- Limited to provided dataset examples
- Sequential training (no parallelization)

## Development

```bash
# Run development server
npm run dev

# Build for production
npm run build

# Start production server
npm start

# Run linter
npm run lint
```

## License

MIT
